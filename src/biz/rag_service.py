"""
RAG ä¸šåŠ¡é€»è¾‘å±‚
ç»Ÿä¸€å°è£…æ£€ç´¢ã€é‡æŽ’åºä¸ŽçŸ¥è¯†åº“ç»´æŠ¤é€»è¾‘ã€‚
"""
import json
import logging
import os
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from tools.vector_store import get_vector_store
from tools.reranker_tool import rerank_documents
from tools.bm25_retriever import bm25_retrieve
from tools.question_classifier import classify_question_type, get_retrieval_strategy

logger = logging.getLogger(__name__)

from tools.document_loader import load_document, get_document_info
from tools.text_splitter import split_text_recursive, split_text_by_markdown_structure
from storage.provider import get_storage_provider

class RAGService:
    def __init__(self, collection_name: str = "knowledge_base"):
        self.collection_name = collection_name
        self.provider = get_storage_provider()

    async def ingest_file(self, file_path: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """å…¨æµç¨‹å…¥åº“ï¼šåŠ è½½ -> åˆ†å‰² -> å­˜å‚¨(S3) -> å‘é‡åŒ–(DB)"""
        # 1. åŠ è½½
        content = load_document.invoke({"file_path": file_path})
        
        # 2. åˆ†å‰² (ç»Ÿä¸€è¿”å›žæ¸…ç†åŽçš„æ–‡æœ¬å—åˆ—è¡¨)
        if file_path.endswith(('.md', '.markdown')):
            chunks_str = split_text_by_markdown_structure.invoke({"text": content})
        else:
            chunks_str = split_text_recursive.invoke({"text": content})
        
        # è§£æžå— (ç§»é™¤å·¥å…·ç”Ÿæˆçš„æ ¼å¼åŒ– Header)
        # å·¥å…·è¾“å‡ºæ ¼å¼ä¸º: "--- å— X (Y å­—ç¬¦) ---\nå†…å®¹\n\n"
        import re
        raw_chunks = chunks_str.split("---")
        chunks = []
        for c in raw_chunks:
            c = c.strip()
            if not c: continue
            # è¿‡æ»¤æŽ‰ "å— 1 (123 å­—ç¬¦) ---" è¿™ç§é—ç•™æ¨¡å¼
            clean_c = re.sub(r'^å— \d+ \(.*?\)\s*---', '', c).strip()
            # è¿‡æ»¤æŽ‰å·¥å…·ç”Ÿæˆçš„æ ‡é¢˜è¡Œ
            if clean_c.startswith(("ðŸ“ æ–‡æœ¬åˆ†å‰²ç»“æžœ", "ðŸ“ Markdown ç»“æž„åˆ†å‰²ç»“æžœ", "æ€»å—æ•°:", "å—å¤§å°:", "é‡å :", "åˆ†å‰²è§„åˆ™:", "====")):
                continue
            if clean_c:
                chunks.append(clean_c)
        
        # 3. æŒä¹…åŒ–åŽŸå§‹æ–‡ä»¶ (ACL)
        with open(file_path, "rb") as f:
            object_key = self.provider.ingest_document(f.read(), os.path.basename(file_path), metadata or {})
            
        # 4. å‘é‡åŒ–å…¥åº“
        vector_store = get_vector_store(collection_name=self.collection_name)
        docs = [Document(page_content=c, metadata={**(metadata or {}), "source": os.path.basename(file_path), "object_key": object_key}) for c in chunks]
        vector_store.add_documents(docs)
        
        return {"object_key": object_key, "chunks": len(chunks)}

    def smart_retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼šåˆ†ç±» -> ç­–ç•¥é€‰æ‹© -> æ‰§è¡Œæ£€ç´¢ -> Rerank"""
        # 1. é—®é¢˜åˆ†ç±»
        q_type_json = classify_question_type.invoke({"query": query})
        try:
            q_type_data = json.loads(q_type_json)
            q_type = q_type_data.get("type", "general")
        except:
            q_type = "general"
        
        # 2. èŽ·å–æŽ¨èç­–ç•¥
        strategy_res = get_retrieval_strategy.invoke({"question_type": q_type})
        strategy_data = json.loads(strategy_res)
        
        # ä¿®æ­£: ä»ŽåµŒå¥—çš„ strategy å­—å…¸ä¸­èŽ·å–é…ç½®
        strategy = strategy_data.get("strategy", {})
        method = strategy.get("method", "vector")
        use_rerank = strategy.get("use_rerank", True)
        
        logger.info(f"Query: {query} | Type: {q_type} | Strategy: {method} | Rerank: {use_rerank}")
        
        # 3. æ‰§è¡ŒåŸºç¡€æ£€ç´¢
        docs = []
        if method == "vector":
            vector_store = get_vector_store(collection_name=self.collection_name)
            results = vector_store.similarity_search_with_score(query, k=top_k * 3)
            for doc, score in results:
                doc.metadata["vector_score"] = float(score)
                docs.append(doc)
        elif method == "bm25":
            bm25_res = bm25_retrieve.invoke({"query": query, "top_k": top_k * 3})
            docs = self._parse_json_docs(bm25_res)
        else: # hybrid
            from tools.hybrid_retriever import hybrid_retrieve
            hybrid_res = hybrid_retrieve.invoke({
                "query": query,
                "collection_name": self.collection_name,
                "top_k": top_k * 3
            })
            docs = self._parse_json_docs(hybrid_res)

        # 4. Rerank
        if use_rerank and docs:
            rerank_input = json.dumps([
                {"content": d.page_content, "id": str(i), "metadata": d.metadata} 
                for i, d in enumerate(docs)
            ])
            reranked_res = rerank_documents.invoke({
                "query": query,
                "documents": rerank_input,
                "top_n": top_k
            })
            
            try:
                reranked_docs = json.loads(reranked_res)
                # è½¬æ¢æ ¼å¼ä¸ºä¸€è‡´çš„å­—å…¸åˆ—è¡¨
                return [{"content": d.get("content", ""), "metadata": d.get("metadata", {}), "relevance_score": d.get("relevance_score", 0.0)} for d in reranked_docs]
            except:
                logger.error("Failed to parse rerank results, falling back to original docs")
                return [{"content": d.page_content, "metadata": d.metadata} for d in docs[:top_k]]
            
        return [{"content": d.page_content, "metadata": d.metadata} for d in docs[:top_k]]

    def get_heatmap(self, topic_level: int = 3, min_frequency: int = 1) -> Dict[str, Any]:
        """èŽ·å–çŸ¥è¯†çƒ­åŠ›å›¾æ•°æ®"""
        from tools.knowledge_heatmap import generate_knowledge_heatmap
        res = generate_knowledge_heatmap.invoke({
            "collection_name": self.collection_name,
            "topic_level": topic_level,
            "min_frequency": min_frequency
        })
        return json.loads(res)

    def get_hierarchy(self, doc_id: str) -> Dict[str, Any]:
        """èŽ·å–æ–‡æ¡£åˆ†å±‚ç»“æž„"""
        from tools.document_hierarchy import build_document_hierarchy
        res = build_document_hierarchy.invoke({
            "document_id": doc_id,
            "collection_name": self.collection_name
        })
        return json.loads(res)

    def _parse_json_docs(self, json_str: str) -> List[Document]:
        """è§£æžå·¥å…·è¿”å›žçš„ JSON å­—ç¬¦ä¸²ä¸º Document åˆ—è¡¨"""
        try:
            if not json_str: return []
            data = json.loads(json_str)
            
            if isinstance(data, list):
                docs_data = data
            elif isinstance(data, dict):
                docs_data = data.get("documents", data.get("results", data.get("final_results", [])))
            else:
                return []

            docs = []
            for d in docs_data:
                content = d.get("content") or d.get("page_content") or d.get("document") or ""
                metadata = d.get("metadata") or {}
                if content:
                    docs.append(Document(page_content=content, metadata=metadata))
            return docs
        except Exception as e:
            logger.error(f"Error parsing JSON docs: {e}")
            return []

    def batch_retrieve(self, queries: List[str], top_k: int = 5) -> Dict[str, Any]:
        """æ‰¹é‡æ£€ç´¢"""
        results = []
        for query in queries:
            try:
                res = self.smart_retrieve(query, top_k)
                results.append({"query": query, "results": res, "count": len(res)})
            except Exception as e:
                results.append({"query": query, "error": str(e)})
        return {"total": len(queries), "results": results}

    def get_statistics(self, queries: List[str]) -> Dict[str, Any]:
        """æ£€ç´¢ç»Ÿè®¡"""
        # ç®€åŒ–ç‰ˆç»Ÿè®¡
        batch_res = self.batch_retrieve(queries)
        stats = {
            "total": batch_res["total"],
            "success": sum(1 for r in batch_res["results"] if "error" not in r),
            "failed": sum(1 for r in batch_res["results"] if "error" in r)
        }
        return stats

    def get_traceability(self, query: str) -> List[Dict[str, Any]]:
        """èŽ·å–ç­”æ¡ˆæº¯æºä¿¡æ¯"""
        results = self.smart_retrieve(query, top_k=5)
        trace_results = []
        for i, item in enumerate(results):
            metadata = item.get("metadata", {})
            raw_score = metadata.get("relevance_score") or metadata.get("vector_score") or metadata.get("score") or 0.0
            trace_results.append({
                "document_name": metadata.get("source") or metadata.get("original_name") or f"æ–‡æ¡£_{i+1}",
                "content": item.get("content", ""),
                "score": item.get("relevance_score", item.get("vector_score", raw_score)),
                "raw_score": raw_score,
                "chunk_index": i
            })
        return trace_results

    def compare_methods(self, query: str, methods: Dict[str, bool]) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸åŒæ£€ç´¢æ–¹æ³•çš„ç»“æžœ"""
        comparison = {}
        top_k = 5
        
        if methods.get('vector'):
            vector_store = get_vector_store(collection_name=self.collection_name)
            results = vector_store.similarity_search_with_score(query, k=top_k)
            scores = [float(s) for d, s in results] if results else []
            avg = sum(scores) / len(scores) if scores else 0.0
            comparison['vector'] = {
                "results": [{"content": d.page_content, "score": float(s), "metadata": d.metadata} for d, s in results],
                "avg_score": avg,
                "time": 0 # ç®€åŒ–å¤„ç†
            }
            
        if methods.get('bm25'):
            bm25_res = bm25_retrieve.invoke({"query": query, "top_k": top_k})
            docs = self._parse_json_docs(bm25_res)
            scores = []
            normalized = []
            for d in docs:
                sc = d.metadata.get("score") if hasattr(d, "metadata") and d.metadata else 0.0
                scores.append(sc)
                normalized.append({"content": d.page_content, "metadata": d.metadata, "score": sc})
            avg = sum(scores) / len(scores) if scores else 0.0
            comparison['bm25'] = {
                "results": normalized,
                "avg_score": avg,
                "time": 0
            }
            
        if methods.get('hybrid'):
            from tools.hybrid_retriever import hybrid_retrieve
            hybrid_res = hybrid_retrieve.invoke({"query": query, "collection_name": self.collection_name, "top_k": top_k})
            docs = self._parse_json_docs(hybrid_res)
            scores = []
            normalized = []
            for d in docs:
                sc = d.metadata.get("score") if hasattr(d, "metadata") and d.metadata else 0.0
                scores.append(sc)
                normalized.append({"content": d.page_content, "metadata": d.metadata, "score": sc})
            avg = sum(scores) / len(scores) if scores else 0.0
            comparison['hybrid'] = {
                "results": normalized,
                "avg_score": avg,
                "time": 0
            }
            
        return comparison

_rag_service = None

def get_rag_service() -> RAGService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service
