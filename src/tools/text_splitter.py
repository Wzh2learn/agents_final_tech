"""
æ–‡æœ¬åˆ†å‰²å·¥å…·
æ”¯æŒé€’å½’åˆ†å‰²å’Œ Markdown ç»“æ„åˆ†å‰²
"""
from typing import List, Optional, Dict, Any
from langchain.tools import tool
from langchain_core.documents import Document


def __dynamic_import():
    """åŠ¨æ€å¯¼å…¥æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé¿å…é™æ€ç±»å‹æ£€æŸ¥é”™è¯¯"""
    # é€’å½’å­—ç¬¦åˆ†å‰²å™¨
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        _recursive_splitter = RecursiveCharacterTextSplitter
    except ImportError:
        _recursive_splitter = None

    # Markdown æ ‡é¢˜åˆ†å‰²å™¨
    try:
        from langchain_text_splitters import MarkdownHeaderTextSplitter
        _markdown_splitter = MarkdownHeaderTextSplitter
    except ImportError:
        _markdown_splitter = None

    return _recursive_splitter, _markdown_splitter


_RecursiveSplitter, _MarkdownSplitter = __dynamic_import()


@tool
def split_text_recursive(
    text: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    separators: Optional[List[str]] = None
) -> str:
    """
    ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨åˆ†å‰²æ–‡æœ¬ï¼ˆæ¨èç”¨äºé€šç”¨æ–‡æœ¬ï¼‰

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 1000ï¼‰
        chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 200ï¼‰
        separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["\n\n", "\n", " ", ""]

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨ï¼ˆå¸¦ç´¢å¼•ï¼‰

    Raises:
        ValueError: å¦‚æœåˆ†å‰²å™¨æœªå®‰è£…æˆ–æ–‡æœ¬ä¸ºç©º
    """
    if _RecursiveSplitter is None:
        raise ValueError(
            "æ–‡æœ¬åˆ†å‰²å™¨æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: "
            "pip install langchain-text-splitters"
        )

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    if separators is None:
        separators = ["\n\n", "\n", " ", ""]

    try:
        splitter = _RecursiveSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators
        )

        # å°†æ–‡æœ¬è½¬æ¢ä¸º Document å¯¹è±¡
        document = Document(page_content=text)

        # åˆ†å‰²æ–‡æ¡£
        chunks = splitter.split_documents([document])

        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ“ æ–‡æœ¬åˆ†å‰²ç»“æœ\n"
        result += f"æ€»å—æ•°: {len(chunks)}\n"
        result += f"å—å¤§å°: {chunk_size} å­—ç¬¦\n"
        result += f"é‡å : {chunk_overlap} å­—ç¬¦\n"
        result += "=" * 50 + "\n\n"

        for i, chunk in enumerate(chunks, 1):
            result += f"--- å— {i} ({len(chunk.page_content)} å­—ç¬¦) ---\n"
            result += f"{chunk.page_content}\n\n"

        return result

    except Exception as e:
        raise ValueError(f"æ–‡æœ¬åˆ†å‰²å¤±è´¥: {str(e)}")


@tool
def split_text_by_markdown_structure(
    text: str,
    headers_to_split_on: Optional[List[tuple]] = None,
    return_each_line: Optional[bool] = False
) -> str:
    """
    åŸºäº Markdown æ ‡é¢˜ç»“æ„åˆ†å‰²æ–‡æœ¬

    Args:
        text: Markdown æ–‡æœ¬
        headers_to_split_on: è¦åˆ†å‰²çš„æ ‡é¢˜åˆ—è¡¨
            æ ¼å¼: [("æ ‡é¢˜å", "æ ‡é¢˜çº§åˆ«")]
            ä¾‹å¦‚: [("#", "Header 1"), ("##", "Header 2")]
            å¦‚æœä¸º Noneï¼Œé»˜è®¤ä½¿ç”¨å¸¸è§æ ‡é¢˜
        return_each_line: æ˜¯å¦è¿”å›æ¯è¡Œå†…å®¹ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—ï¼ˆå¸¦æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼‰

    Raises:
        ValueError: å¦‚æœåˆ†å‰²å™¨æœªå®‰è£…æˆ–æ–‡æœ¬ä¸ºç©º
    """
    if _MarkdownSplitter is None:
        raise ValueError(
            "Markdown åˆ†å‰²å™¨æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: "
            "pip install langchain-text-splitters"
        )

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    if headers_to_split_on is None:
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]

    try:
        splitter = _MarkdownSplitter(
            headers_to_split_on=headers_to_split_on
        )

        # åˆ†å‰²æ–‡æ¡£ï¼ˆä½¿ç”¨split_textæ–¹æ³•ï¼‰
        chunks = splitter.split_text(text)

        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ“ Markdown ç»“æ„åˆ†å‰²ç»“æœ\n"
        result += f"æ€»å—æ•°: {len(chunks)}\n"
        result += f"åˆ†å‰²è§„åˆ™: {[h[0] for h in headers_to_split_on]}\n"
        result += "=" * 50 + "\n\n"

        for i, chunk in enumerate(chunks, 1):
            result += f"--- å— {i} ({len(chunk)} å­—ç¬¦) ---\n"
            result += f"{chunk}\n\n"

        return result

    except Exception as e:
        raise ValueError(f"Markdown åˆ†å‰²å¤±è´¥: {str(e)}")


@tool
def split_document_optimized(
    text: str,
    file_type: str = "text"
) -> str:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åˆ†å‰²ç­–ç•¥

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        file_type: æ–‡ä»¶ç±»å‹
            - "text": é€šç”¨æ–‡æœ¬ï¼ˆä½¿ç”¨é€’å½’åˆ†å‰²ï¼‰
            - "markdown": Markdown æ–‡æ¡£ï¼ˆä½¿ç”¨æ ‡é¢˜ç»“æ„åˆ†å‰²ï¼‰
            - "code": ä»£ç æ–‡ä»¶ï¼ˆä½¿ç”¨é€’å½’åˆ†å‰²ï¼Œå°å—ï¼‰

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—

    Raises:
        ValueError: å¦‚æœæ–‡ä»¶ç±»å‹ä¸æ”¯æŒ
    """
    # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆ†å‰²ç­–ç•¥
    if file_type == "markdown":
        return split_text_by_markdown_structure(text)
    elif file_type == "code":
        # ä»£ç ä½¿ç”¨è¾ƒå°çš„å—
        return split_text_recursive(
            text,
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "  ", ". ", " "]
        )
    else:
        # é»˜è®¤ä½¿ç”¨é€šç”¨æ–‡æœ¬åˆ†å‰²
        return split_text_recursive(text)


@tool
def split_text_with_summary(
    text: str,
    max_chunks: Optional[int] = None
) -> str:
    """
    åˆ†å‰²æ–‡æœ¬å¹¶ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        max_chunks: æœ€å¤§åˆ†å‰²å—æ•°ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰

    Returns:
        åˆ†å‰²ç»“æœ + ç»Ÿè®¡æ‘˜è¦

    Raises:
        ValueError: å¦‚æœæ–‡æœ¬ä¸ºç©º
    """
    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    # ä½¿ç”¨é€’å½’åˆ†å‰²
    result = split_text_recursive(text)

    # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
    total_chars = len(text)
    total_words = len(text.split())
    total_lines = len(text.split('\n'))

    summary = f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦\n"
    summary += "=" * 30 + "\n"
    summary += f"æ€»å­—ç¬¦æ•°: {total_chars}\n"
    summary += f"æ€»è¯æ•°: {total_words}\n"
    summary += f"æ€»è¡Œæ•°: {total_lines}\n"
    summary += f"å¹³å‡å—å¤§å°: {total_chars // max(len(result.split('---')) - 1, 1)} å­—ç¬¦\n"

    return result + summary


def hierarchical_split(
    text: str,
    parent_chunk_size: int = 2000,
    child_chunk_size: int = 500,
    chunk_overlap: int = 100
) -> List[Dict[str, any]]:
    """
    çˆ¶å­åˆ†æ®µæ¨¡å¼ï¼šä¸¤çº§åˆ†å‰²ï¼Œçˆ¶å—ç”¨äºæ¦‚è§ˆï¼Œå­å—ç”¨äºè¯¦ç»†æ£€ç´¢
    
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        parent_chunk_size: çˆ¶å—å¤§å°ï¼ˆé»˜è®¤2000å­—ç¬¦ï¼‰
        child_chunk_size: å­å—å¤§å°ï¼ˆé»˜è®¤500å­—ç¬¦ï¼‰
        chunk_overlap: å—ä¹‹é—´é‡å ï¼ˆé»˜è®¤100å­—ç¬¦ï¼‰
    
    Returns:
        åŒ…å«çˆ¶å­å…³ç³»çš„Documentåˆ—è¡¨
        æ¯ä¸ªçˆ¶DocumentåŒ…å«metadata: {"parent_id": str, "is_parent": True}
        æ¯ä¸ªå­DocumentåŒ…å«metadata: {"parent_id": str, "is_parent": False, "child_index": int}
    """
    if _RecursiveSplitter is None:
        raise ValueError("æ–‡æœ¬åˆ†å‰²å™¨æœªå®‰è£…")
    
    if not text or not text.strip():
        return []
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºçˆ¶å—
    parent_splitter = _RecursiveSplitter(
        chunk_size=parent_chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    
    parent_docs = parent_splitter.split_documents([Document(page_content=text)])
    
    # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªçˆ¶å—åˆ›å»ºå­å—
    all_chunks = []
    
    for parent_idx, parent_doc in enumerate(parent_docs):
        parent_id = f"parent_{parent_idx}"
        
        # æ·»åŠ çˆ¶å—ï¼ˆç”¨äºæ¦‚è§ˆï¼‰
        parent_doc.metadata.update({
            "parent_id": parent_id,
            "is_parent": True,
            "chunk_index": parent_idx
        })
        all_chunks.append(parent_doc)
        
        # åˆ›å»ºå­å—åˆ†å‰²å™¨
        child_splitter = _RecursiveSplitter(
            chunk_size=child_chunk_size,
            chunk_overlap=chunk_overlap // 2,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]
        )
        
        # ä»çˆ¶å—å†…å®¹åˆ›å»ºå­å—
        child_docs = child_splitter.split_documents([parent_doc])
        
        # ä¸ºå­å—æ·»åŠ å…ƒæ•°æ®
        for child_idx, child_doc in enumerate(child_docs):
            child_doc.metadata.update({
                "parent_id": parent_id,
                "is_parent": False,
                "child_index": child_idx,
                "chunk_index": f"{parent_idx}_{child_idx}"
            })
            all_chunks.append(child_doc)
    
    return all_chunks


@tool
def split_text_hierarchical(
    text: str,
    parent_chunk_size: int = 2000,
    child_chunk_size: int = 500
) -> str:
    """
    ä½¿ç”¨çˆ¶å­åˆ†æ®µæ¨¡å¼åˆ†å‰²æ–‡æœ¬ï¼ˆå·¥å…·åŒ…è£…ï¼‰
    
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        parent_chunk_size: çˆ¶å—å¤§å°
        child_chunk_size: å­å—å¤§å°
    
    Returns:
        æ ¼å¼åŒ–çš„åˆ†å‰²ç»“æœ
    """
    try:
        chunks = hierarchical_split(text, parent_chunk_size, child_chunk_size)
        
        result = f"ğŸ“ çˆ¶å­åˆ†æ®µç»“æœ\n"
        result += f"çˆ¶å—å¤§å°: {parent_chunk_size} | å­å—å¤§å°: {child_chunk_size}\n"
        result += f"æ€»å—æ•°: {len(chunks)}\n"
        result += "=" * 50 + "\n\n"
        
        for chunk in chunks:
            is_parent = chunk.metadata.get("is_parent", False)
            chunk_type = "ã€çˆ¶å—ã€‘" if is_parent else "  ã€å­å—ã€‘"
            chunk_id = chunk.metadata.get("chunk_index", "")
            
            result += f"{chunk_type} ID: {chunk_id} ({len(chunk.page_content)} å­—ç¬¦)\n"
            result += f"{chunk.page_content[:100]}...\n\n"
        
        return result
    except Exception as e:
        raise ValueError(f"çˆ¶å­åˆ†æ®µå¤±è´¥: {str(e)}")
