"""
æ–‡æœ¬åˆ†å‰²å·¥å…·
æ”¯æŒé€’å½’åˆ†å‰²å’Œ Markdown ç»“æ„åˆ†å‰²
"""
from typing import List, Optional
from langchain.tools import tool
from langchain_core.documents import Document


def __dynamic_import():
    """åŠ¨æ€å¯¼å…¥æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé¿å…é™æ€ç±»å‹æ£€æŸ¥é”™è¯¯"""
    # é€’å½’å­—ç¬¦åˆ†å‰²å™¨
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        _recursive_splitter = RecursiveCharacterTextSplitter
    except ImportError:
        _recursive_splitter = None

    # Markdown æ ‡é¢˜åˆ†å‰²å™¨
    try:
        from langchain_text_splitters import MarkdownHeaderTextSplitter
        _markdown_splitter = MarkdownHeaderTextSplitter
    except ImportError:
        _markdown_splitter = None

    return _recursive_splitter, _markdown_splitter


_RecursiveSplitter, _MarkdownSplitter = __dynamic_import()


@tool
def split_text_recursive(
    text: str,
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    separators: Optional[List[str]] = None
) -> str:
    """
    ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨åˆ†å‰²æ–‡æœ¬ï¼ˆæ¨èç”¨äºé€šç”¨æ–‡æœ¬ï¼‰

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 1000ï¼‰
        chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 200ï¼‰
        separators: åˆ†éš”ç¬¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["\n\n", "\n", " ", ""]

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨ï¼ˆå¸¦ç´¢å¼•ï¼‰

    Raises:
        ValueError: å¦‚æœåˆ†å‰²å™¨æœªå®‰è£…æˆ–æ–‡æœ¬ä¸ºç©º
    """
    if _RecursiveSplitter is None:
        raise ValueError(
            "æ–‡æœ¬åˆ†å‰²å™¨æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: "
            "pip install langchain-text-splitters"
        )

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    if separators is None:
        separators = ["\n\n", "\n", " ", ""]

    try:
        splitter = _RecursiveSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators
        )

        # å°†æ–‡æœ¬è½¬æ¢ä¸º Document å¯¹è±¡
        document = Document(page_content=text)

        # åˆ†å‰²æ–‡æ¡£
        chunks = splitter.split_documents([document])

        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ“ æ–‡æœ¬åˆ†å‰²ç»“æœ\n"
        result += f"æ€»å—æ•°: {len(chunks)}\n"
        result += f"å—å¤§å°: {chunk_size} å­—ç¬¦\n"
        result += f"é‡å : {chunk_overlap} å­—ç¬¦\n"
        result += "=" * 50 + "\n\n"

        for i, chunk in enumerate(chunks, 1):
            result += f"--- å— {i} ({len(chunk.page_content)} å­—ç¬¦) ---\n"
            result += f"{chunk.page_content}\n\n"

        return result

    except Exception as e:
        raise ValueError(f"æ–‡æœ¬åˆ†å‰²å¤±è´¥: {str(e)}")


@tool
def split_text_by_markdown_structure(
    text: str,
    headers_to_split_on: Optional[List[tuple]] = None,
    return_each_line: Optional[bool] = False
) -> str:
    """
    åŸºäº Markdown æ ‡é¢˜ç»“æ„åˆ†å‰²æ–‡æœ¬

    Args:
        text: Markdown æ–‡æœ¬
        headers_to_split_on: è¦åˆ†å‰²çš„æ ‡é¢˜åˆ—è¡¨
            æ ¼å¼: [("æ ‡é¢˜å", "æ ‡é¢˜çº§åˆ«")]
            ä¾‹å¦‚: [("#", "Header 1"), ("##", "Header 2")]
            å¦‚æœä¸º Noneï¼Œé»˜è®¤ä½¿ç”¨å¸¸è§æ ‡é¢˜
        return_each_line: æ˜¯å¦è¿”å›æ¯è¡Œå†…å®¹ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—ï¼ˆå¸¦æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼‰

    Raises:
        ValueError: å¦‚æœåˆ†å‰²å™¨æœªå®‰è£…æˆ–æ–‡æœ¬ä¸ºç©º
    """
    if _MarkdownSplitter is None:
        raise ValueError(
            "Markdown åˆ†å‰²å™¨æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: "
            "pip install langchain-text-splitters"
        )

    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    if headers_to_split_on is None:
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]

    try:
        splitter = _MarkdownSplitter(
            headers_to_split_on=headers_to_split_on
        )

        # åˆ†å‰²æ–‡æ¡£ï¼ˆä½¿ç”¨split_textæ–¹æ³•ï¼‰
        chunks = splitter.split_text(text)

        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ“ Markdown ç»“æ„åˆ†å‰²ç»“æœ\n"
        result += f"æ€»å—æ•°: {len(chunks)}\n"
        result += f"åˆ†å‰²è§„åˆ™: {[h[0] for h in headers_to_split_on]}\n"
        result += "=" * 50 + "\n\n"

        for i, chunk in enumerate(chunks, 1):
            result += f"--- å— {i} ({len(chunk)} å­—ç¬¦) ---\n"
            result += f"{chunk}\n\n"

        return result

    except Exception as e:
        raise ValueError(f"Markdown åˆ†å‰²å¤±è´¥: {str(e)}")


@tool
def split_document_optimized(
    text: str,
    file_type: str = "text"
) -> str:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åˆ†å‰²ç­–ç•¥

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        file_type: æ–‡ä»¶ç±»å‹
            - "text": é€šç”¨æ–‡æœ¬ï¼ˆä½¿ç”¨é€’å½’åˆ†å‰²ï¼‰
            - "markdown": Markdown æ–‡æ¡£ï¼ˆä½¿ç”¨æ ‡é¢˜ç»“æ„åˆ†å‰²ï¼‰
            - "code": ä»£ç æ–‡ä»¶ï¼ˆä½¿ç”¨é€’å½’åˆ†å‰²ï¼Œå°å—ï¼‰

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—

    Raises:
        ValueError: å¦‚æœæ–‡ä»¶ç±»å‹ä¸æ”¯æŒ
    """
    # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆ†å‰²ç­–ç•¥
    if file_type == "markdown":
        return split_text_by_markdown_structure(text)
    elif file_type == "code":
        # ä»£ç ä½¿ç”¨è¾ƒå°çš„å—
        return split_text_recursive(
            text,
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "  ", ". ", " "]
        )
    else:
        # é»˜è®¤ä½¿ç”¨é€šç”¨æ–‡æœ¬åˆ†å‰²
        return split_text_recursive(text)


@tool
def split_text_with_summary(
    text: str,
    max_chunks: Optional[int] = None
) -> str:
    """
    åˆ†å‰²æ–‡æœ¬å¹¶ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        max_chunks: æœ€å¤§åˆ†å‰²å—æ•°ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰

    Returns:
        åˆ†å‰²ç»“æœ + ç»Ÿè®¡æ‘˜è¦

    Raises:
        ValueError: å¦‚æœæ–‡æœ¬ä¸ºç©º
    """
    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    # ä½¿ç”¨é€’å½’åˆ†å‰²
    result = split_text_recursive(text)

    # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
    total_chars = len(text)
    total_words = len(text.split())
    total_lines = len(text.split('\n'))

    summary = f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦\n"
    summary += "=" * 30 + "\n"
    summary += f"æ€»å­—ç¬¦æ•°: {total_chars}\n"
    summary += f"æ€»è¯æ•°: {total_words}\n"
    summary += f"æ€»è¡Œæ•°: {total_lines}\n"
    summary += f"å¹³å‡å—å¤§å°: {total_chars // max(len(result.split('---')) - 1, 1)} å­—ç¬¦\n"

    return result + summary
