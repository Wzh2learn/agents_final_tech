"""
æ··åˆæ£€ç´¢ç­–ç•¥å·¥å…·
ç»“åˆå‘é‡æ£€ç´¢å’ŒBM25å…¨æ–‡æ£€ç´¢ï¼Œå®ç°æ›´ç²¾ç¡®çš„æ£€ç´¢
"""
import json
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from langchain.tools import tool

# å¯¼å…¥ç›¸å…³å·¥å…·
from tools.rag_retriever import rag_retrieve_with_rerank
from tools.bm25_retriever import bm25_retrieve
from tools.reranker_tool import rerank_documents


def _normalize_scores(scores: List[float], method: str = "minmax") -> List[float]:
    """
    å½’ä¸€åŒ–åˆ†æ•°åˆ°[0, 1]åŒºé—´

    Args:
        scores: åŸå§‹åˆ†æ•°åˆ—è¡¨
        method: å½’ä¸€åŒ–æ–¹æ³•ï¼ˆminmax=æœ€å°-æœ€å¤§å½’ä¸€åŒ–ï¼Œsigmoid=Sigmoidå½’ä¸€åŒ–ï¼‰

    Returns:
        å½’ä¸€åŒ–åçš„åˆ†æ•°åˆ—è¡¨
    """
    if not scores:
        return []

    scores = np.array(scores)

    if method == "minmax":
        # æœ€å°-æœ€å¤§å½’ä¸€åŒ–
        min_score = scores.min()
        max_score = scores.max()
        if max_score - min_score > 0:
            normalized = (scores - min_score) / (max_score - min_score)
        else:
            normalized = np.ones_like(scores) * 0.5
    elif method == "sigmoid":
        # Sigmoidå½’ä¸€åŒ–ï¼ˆé€‚åˆåˆ†æ•°èŒƒå›´è¾ƒå¤§çš„æƒ…å†µï¼‰
        normalized = 1 / (1 + np.exp(-scores))
    else:
        # é»˜è®¤ä½¿ç”¨minmax
        return _normalize_scores(scores, "minmax")

    return normalized.tolist()


def _merge_results(
    vector_results: List[Dict[str, Any]],
    bm25_results: List[Dict[str, Any]],
    vector_weight: float,
    bm25_weight: float
) -> List[Dict[str, Any]]:
    """
    èåˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢çš„ç»“æœ

    Args:
        vector_results: å‘é‡æ£€ç´¢ç»“æœ
        bm25_results: BM25æ£€ç´¢ç»“æœ
        vector_weight: å‘é‡æ£€ç´¢æƒé‡
        bm25_weight: BM25æ£€ç´¢æƒé‡

    Returns:
        èåˆåçš„ç»“æœåˆ—è¡¨
    """
    # åˆ›å»ºæ–‡æ¡£IDåˆ°ç»“æœçš„æ˜ å°„
    vector_map = {}
    for i, result in enumerate(vector_results):
        # ä½¿ç”¨æ–‡æ¡£å†…å®¹æˆ–ç´¢å¼•ä½œä¸ºå”¯ä¸€æ ‡è¯†
        doc_id = result.get("document", "").strip()[:50]  # ä½¿ç”¨å‰50ä¸ªå­—ç¬¦ä½œä¸ºID
        vector_map[doc_id] = {
            "result": result,
            "vector_score": result.get("vector_score", result.get("score", 0)),
            "bm25_score": 0.0,
            "index": i
        }

    bm25_map = {}
    for i, result in enumerate(bm25_results):
        doc_id = result.get("document", "").strip()[:50]
        bm25_map[doc_id] = {
            "result": result,
            "bm25_score": result.get("bm25_score", result.get("score", 0)),
            "vector_score": 0.0,
            "index": i
        }

    # åˆå¹¶ä¸¤ä¸ªæ˜ å°„
    all_doc_ids = set(vector_map.keys()) | set(bm25_map.keys())

    merged_results = []

    for doc_id in all_doc_ids:
        if doc_id in vector_map and doc_id in bm25_map:
            # æ–‡æ¡£åœ¨ä¸¤ä¸ªæ£€ç´¢ç»“æœä¸­éƒ½å­˜åœ¨
            vector_data = vector_map[doc_id]
            bm25_data = bm25_map[doc_id]

            merged_results.append({
                "document": vector_data["result"].get("document", ""),
                "metadata": vector_data["result"].get("metadata", {}),
                "vector_score": float(vector_data["vector_score"]),
                "bm25_score": float(bm25_data["bm25_score"]),
                "vector_rank": vector_data["index"],
                "bm25_rank": bm25_data["index"]
            })

        elif doc_id in vector_map:
            # æ–‡æ¡£åªåœ¨å‘é‡æ£€ç´¢ç»“æœä¸­
            vector_data = vector_map[doc_id]
            merged_results.append({
                "document": vector_data["result"].get("document", ""),
                "metadata": vector_data["result"].get("metadata", {}),
                "vector_score": float(vector_data["vector_score"]),
                "bm25_score": 0.0,
                "vector_rank": vector_data["index"],
                "bm25_rank": -1
            })

        else:
            # æ–‡æ¡£åªåœ¨BM25æ£€ç´¢ç»“æœä¸­
            bm25_data = bm25_map[doc_id]
            merged_results.append({
                "document": bm25_data["result"].get("document", ""),
                "metadata": bm25_data["result"].get("metadata", {}),
                "vector_score": 0.0,
                "bm25_score": float(bm25_data["bm25_score"]),
                "vector_rank": -1,
                "bm25_rank": bm25_data["index"]
            })

    return merged_results


def _calculate_hybrid_score(
    merged_results: List[Dict[str, Any]],
    vector_weight: float,
    bm25_weight: float,
    score_method: str = "weighted"
) -> List[Tuple[int, float]]:
    """
    è®¡ç®—æ··åˆæ£€ç´¢åˆ†æ•°

    Args:
        merged_results: èåˆåçš„ç»“æœåˆ—è¡¨
        vector_weight: å‘é‡æ£€ç´¢æƒé‡
        bm25_weight: BM25æ£€ç´¢æƒé‡
        score_method: åˆ†æ•°è®¡ç®—æ–¹æ³•

    Returns:
        (ç´¢å¼•, åˆ†æ•°)çš„æ’åºåˆ—è¡¨
    """
    vector_scores = [r.get("vector_score", 0) for r in merged_results]
    bm25_scores = [r.get("bm25_score", 0) for r in merged_results]

    # å½’ä¸€åŒ–åˆ†æ•°
    vector_normalized = _normalize_scores(vector_scores)
    bm25_normalized = _normalize_scores(bm25_scores)

    hybrid_scores = []

    for i, result in enumerate(merged_results):
        vec_score = vector_normalized[i]
        bm25_score = bm25_normalized[i]

        if score_method == "weighted":
            # åŠ æƒå¹³å‡
            hybrid_score = vec_score * vector_weight + bm25_score * bm25_weight

        elif score_method == "rrf":
            # Reciprocal Rank Fusionï¼ˆå€’æ•°æ’åèåˆï¼‰
            k = 60  # RRFå¸¸æ•°
            vec_rank = result.get("vector_rank", -1)
            bm25_rank = result.get("bm25_rank", -1)

            vec_rrf = 1 / (k + vec_rank + 1) if vec_rank >= 0 else 0
            bm25_rrf = 1 / (k + bm25_rank + 1) if bm25_rank >= 0 else 0

            hybrid_score = vec_rrf * vector_weight + bm25_rrf * bm25_weight

        else:
            # é»˜è®¤ä½¿ç”¨åŠ æƒå¹³å‡
            hybrid_score = vec_score * vector_weight + bm25_score * bm25_weight

        result["hybrid_score"] = float(hybrid_score)
        hybrid_scores.append((i, hybrid_score))

    # æŒ‰åˆ†æ•°é™åºæ’åº
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)

    return hybrid_scores


def _parse_vector_retrieval_result(result_str: str) -> List[Dict[str, Any]]:
    """
    è§£æå‘é‡æ£€ç´¢ç»“æœå­—ç¬¦ä¸²

    Args:
        result_str: å‘é‡æ£€ç´¢å·¥å…·è¿”å›çš„å­—ç¬¦ä¸²

    Returns:
        è§£æåçš„æ–‡æ¡£åˆ—è¡¨
    """
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¿”å›çš„å­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œè§£æ
    # ç”±äºrag_retrieve_with_rerankè¿”å›çš„æ˜¯æ ¼å¼åŒ–æ–‡æœ¬ï¼Œè¿™é‡Œåšç®€åŒ–å¤„ç†
    # å®é™…åº”è¯¥æ”¹è¿›å‘é‡æ£€ç´¢å·¥å…·ï¼Œä½¿å…¶è¿”å›ç»“æ„åŒ–çš„JSON

    # ç®€åŒ–å¤„ç†ï¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…éœ€è¦è§£æ
    return []


def _get_vector_retrieval_documents(
    query: str,
    collection_name: str,
    initial_k: int
) -> List[Dict[str, Any]]:
    """
    è·å–å‘é‡æ£€ç´¢çš„æ–‡æ¡£

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        collection_name: é›†åˆåç§°
        initial_k: åˆå§‹æ£€ç´¢æ•°é‡

    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    try:
        # è·å–å‘é‡å­˜å‚¨
        from tools.vector_store import get_vector_store
        vector_store = get_vector_store(collection_name=collection_name)

        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        results = vector_store.similarity_search_with_score(
            query=query,
            k=initial_k
        )

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        documents = []
        for doc, score in results:
            documents.append({
                "text": doc.page_content,
                "document": doc.page_content,
                "metadata": doc.metadata,
                "vector_score": float(score),
                "score": float(score)
            })

        return documents

    except Exception as e:
        print(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
        return []


@tool
def hybrid_retrieve(
    query: str,
    documents: str = "[]",
    collection_name: Optional[str] = "knowledge_base",
    top_k: Optional[int] = 5,
    vector_weight: Optional[float] = 0.5,
    bm25_weight: Optional[float] = 0.5,
    score_method: Optional[str] = "weighted",
    use_rerank: Optional[bool] = False
) -> str:
    """
    æ··åˆæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢ + BM25å…¨æ–‡æ£€ç´¢ + å¯é€‰Rerankï¼‰

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰ï¼Œç”¨äºBM25æ£€ç´¢
        collection_name: å‘é‡é›†åˆåç§°ï¼Œç”¨äºå‘é‡æ£€ç´¢
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        vector_weight: å‘é‡æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼Œé»˜è®¤0.5ï¼‰
        bm25_weight: BM25æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼Œé»˜è®¤0.5ï¼‰
        score_method: èåˆæ–¹æ³•ï¼ˆweighted=åŠ æƒå¹³å‡ï¼Œrrf=å€’æ•°æ’åèåˆï¼‰
        use_rerank: æ˜¯å¦ä½¿ç”¨Reranké‡æ’åº

    Returns:
        JSON æ ¼å¼çš„æ··åˆæ£€ç´¢ç»“æœ
    """
    if not query or not query.strip():
        raise ValueError("æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")

    # å½’ä¸€åŒ–æƒé‡
    total_weight = vector_weight + bm25_weight
    if total_weight > 0:
        vector_weight = vector_weight / total_weight
        bm25_weight = bm25_weight / total_weight

    # åˆå§‹åŒ–ç»“æœ
    results = {
        "query": query,
        "method": "hybrid",
        "parameters": {
            "vector_weight": vector_weight,
            "bm25_weight": bm25_weight,
            "top_k": top_k,
            "score_method": score_method,
            "use_rerank": use_rerank
        },
        "vector_count": 0,
        "bm25_count": 0,
        "final_count": 0,
        "results": []
    }

    try:
        # 1. å‘é‡æ£€ç´¢ï¼ˆè·å–æ›´å¤šæ–‡æ¡£ä»¥ä¾¿èåˆï¼‰
        initial_k = min(top_k * 3, 50)  # è·å–3å€çš„æ–‡æ¡£ç”¨äºèåˆ
        vector_docs = _get_vector_retrieval_documents(query, collection_name, initial_k)
        results["vector_count"] = len(vector_docs)

        # 2. BM25æ£€ç´¢
        from tools.bm25_retriever import bm25_retrieve as bm25_retrieve_func
        bm25_result_str = bm25_retrieve_func(
            query=query,
            documents=documents,
            collection_name=collection_name,
            top_k=initial_k
        )
        bm25_result = json.loads(bm25_result_str)
        bm25_docs = bm25_result.get("results", [])
        results["bm25_count"] = len(bm25_docs)

        # 3. èåˆç»“æœ
        merged_results = _merge_results(vector_docs, bm25_docs, vector_weight, bm25_weight)

        # 4. è®¡ç®—æ··åˆåˆ†æ•°
        if merged_results:
            hybrid_scores = _calculate_hybrid_score(
                merged_results,
                vector_weight,
                bm25_weight,
                score_method
            )

            # å–top_kç»“æœ
            final_results = []
            for idx, score in hybrid_scores[:top_k]:
                result = merged_results[idx].copy()
                result["hybrid_rank"] = len(final_results) + 1
                final_results.append(result)

            # 5. å¯é€‰çš„Reranké‡æ’
            if use_rerank and final_results:
                # å‡†å¤‡rerankè¾“å…¥
                rerank_docs = []
                for i, r in enumerate(final_results):
                    rerank_docs.append({
                        "content": r.get("document", ""),
                        "id": str(i),
                        "hybrid_score": r.get("hybrid_score", 0)
                    })

                # è°ƒç”¨rerank
                from tools.reranker_tool import rerank_documents as rerank_func
                rerank_json = rerank_func(
                    query=query,
                    documents=json.dumps(rerank_docs),
                    top_n=top_k
                )
                rerank_results = json.loads(rerank_json)

                # æ›´æ–°æœ€ç»ˆç»“æœ
                for ranked_doc in rerank_results:
                    doc_id = int(ranked_doc.get("id", "0"))
                    if doc_id < len(final_results):
                        final_results[doc_id]["rerank_score"] = ranked_doc.get("relevance_score", 0.5)
                        final_results[doc_id]["rerank_reason"] = ranked_doc.get("reason", "")

                # æŒ‰rerankåˆ†æ•°é‡æ–°æ’åº
                final_results.sort(
                    key=lambda x: x.get("rerank_score", x.get("hybrid_score", 0)),
                    reverse=True
                )

            results["final_results"] = final_results
            results["final_count"] = len(final_results)

        # æ ¼å¼åŒ–è¾“å‡ºç”¨äºå±•ç¤º
        output_text = f"ğŸ” æ··åˆæ£€ç´¢ç»“æœ\n"
        output_text += f"æŸ¥è¯¢: {query}\n"
        output_text += f"å‘é‡æ£€ç´¢æƒé‡: {vector_weight:.2f}\n"
        output_text += f"BM25æ£€ç´¢æƒé‡: {bm25_weight:.2f}\n"
        output_text += f"èåˆæ–¹æ³•: {score_method}\n"
        output_text += f"ä½¿ç”¨Rerank: {'æ˜¯' if use_rerank else 'å¦'}\n"
        output_text += f"å‘é‡æ£€ç´¢: {results['vector_count']} æ–‡æ¡£\n"
        output_text += f"BM25æ£€ç´¢: {results['bm25_count']} æ–‡æ¡£\n"
        output_text += f"æœ€ç»ˆè¿”å›: {results['final_count']} æ–‡æ¡£\n"
        output_text += "=" * 60 + "\n\n"

        for i, result in enumerate(final_results, 1):
            output_text += f"ã€ç»“æœ {i}ã€‘\n"
            output_text += f"å‘é‡åˆ†æ•°: {result.get('vector_score', 0):.4f}\n"
            output_text += f"BM25åˆ†æ•°: {result.get('bm25_score', 0):.4f}\n"
            output_text += f"æ··åˆåˆ†æ•°: {result.get('hybrid_score', 0):.4f}\n"
            if "rerank_score" in result:
                output_text += f"Rerankåˆ†æ•°: {result['rerank_score']:.4f}\n"
                if "rerank_reason" in result and result["rerank_reason"]:
                    output_text += f"RerankåŸå› : {result['rerank_reason']}\n"
            output_text += f"å†…å®¹: {result.get('document', '')[:300]}...\n"
            if result.get("metadata"):
                output_text += f"æ¥æº: {result['metadata'].get('source', 'æœªçŸ¥')}\n"
            output_text += "\n"

        results["summary"] = output_text

        return json.dumps(results, ensure_ascii=False, indent=2)

    except Exception as e:
        results["error"] = f"æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}"
        results["summary"] = f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}"
        return json.dumps(results, ensure_ascii=False, indent=2)


@tool
def compare_retrieval_methods(
    query: str,
    documents: str = "[]",
    collection_name: Optional[str] = "knowledge_base",
    top_k: Optional[int] = 5
) -> str:
    """
    å¯¹æ¯”ä¸åŒæ£€ç´¢æ–¹æ³•çš„ç»“æœ

    å¯¹æ¯”ä»¥ä¸‹æ–¹æ³•ï¼š
    1. å‘é‡æ£€ç´¢
    2. BM25æ£€ç´¢
    3. æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰
    4. æ··åˆæ£€ç´¢+Rerank

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
        collection_name: å‘é‡é›†åˆåç§°
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡

    Returns:
        JSON æ ¼å¼çš„å¯¹æ¯”ç»“æœ
    """
    comparison = {
        "query": query,
        "methods": {}
    }

    try:
        # 1. å‘é‡æ£€ç´¢
        vector_result = _get_vector_retrieval_documents(query, collection_name, top_k)
        comparison["methods"]["vector"] = {
            "count": len(vector_result),
            "top_scores": [r.get("vector_score", 0) for r in vector_result[:3]]
        }

        # 2. BM25æ£€ç´¢
        from tools.bm25_retriever import bm25_retrieve as bm25_func
        bm25_result_str = bm25_func(
            query=query,
            documents=documents,
            collection_name=collection_name,
            top_k=top_k
        )
        bm25_result = json.loads(bm25_result_str)
        bm25_docs = bm25_result.get("results", [])
        comparison["methods"]["bm25"] = {
            "count": len(bm25_docs),
            "top_scores": [r.get("bm25_score", 0) for r in bm25_docs[:3]]
        }

        # 3. æ··åˆæ£€ç´¢ï¼ˆä¸ä½¿ç”¨Rerankï¼‰
        hybrid_result_str = hybrid_retrieve(
            query=query,
            documents=documents,
            collection_name=collection_name,
            top_k=top_k,
            vector_weight=0.5,
            bm25_weight=0.5,
            use_rerank=False
        )
        hybrid_result = json.loads(hybrid_result_str)
        hybrid_docs = hybrid_result.get("final_results", [])
        comparison["methods"]["hybrid"] = {
            "count": len(hybrid_docs),
            "top_scores": [r.get("hybrid_score", 0) for r in hybrid_docs[:3]]
        }

        # 4. æ··åˆæ£€ç´¢+Rerank
        hybrid_rerank_str = hybrid_retrieve(
            query=query,
            documents=documents,
            collection_name=collection_name,
            top_k=top_k,
            vector_weight=0.5,
            bm25_weight=0.5,
            use_rerank=True
        )
        hybrid_rerank_result = json.loads(hybrid_rerank_str)
        hybrid_rerank_docs = hybrid_rerank_result.get("final_results", [])
        comparison["methods"]["hybrid_rerank"] = {
            "count": len(hybrid_rerank_docs),
            "top_scores": [r.get("rerank_score", r.get("hybrid_score", 0)) for r in hybrid_rerank_docs[:3]]
        }

        # ç”Ÿæˆå¯¹æ¯”æ‘˜è¦
        summary = f"ğŸ“Š æ£€ç´¢æ–¹æ³•å¯¹æ¯”\n"
        summary += f"æŸ¥è¯¢: {query}\n"
        summary += f"è¿”å›æ•°é‡: {top_k}\n"
        summary += "=" * 60 + "\n\n"

        for method_name, method_data in comparison["methods"].items():
            summary += f"ã€{method_name.upper()}ã€‘\n"
            summary += f"  æ–‡æ¡£æ•°: {method_data['count']}\n"
            summary += f"  Top-3 åˆ†æ•°: {', '.join([f'{s:.4f}' for s in method_data['top_scores']])}\n"
            summary += "\n"

        comparison["summary"] = summary

        return json.dumps(comparison, ensure_ascii=False, indent=2)

    except Exception as e:
        comparison["error"] = f"å¯¹æ¯”å¤±è´¥: {str(e)}"
        comparison["summary"] = f"âŒ å¯¹æ¯”å¤±è´¥: {str(e)}"
        return json.dumps(comparison, ensure_ascii=False, indent=2)
