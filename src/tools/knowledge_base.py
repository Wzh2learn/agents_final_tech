"""
çŸ¥è¯†åº“ç®¡ç†å·¥å…·
æ”¯æŒæ–‡æ¡£ä¸Šä¼ ã€ç´¢å¼•ã€åˆ é™¤å’ŒæŸ¥è¯¢
"""
import os
import json
from typing import Optional, List, Dict
from langchain.tools import tool
from langchain_core.documents import Document

# å¯¼å…¥ç›¸å…³å·¥å…·
from tools.document_loader import load_document, get_document_info
from tools.text_splitter import split_text_recursive, split_text_by_markdown_structure
from tools.vector_store import get_vector_store, get_embeddings


def __get_file_type(file_path: str) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    ext = os.path.splitext(file_path)[1].lower()
    if ext in ['.md', '.markdown']:
        return "markdown"
    elif ext in ['.docx']:
        return "word"
    else:
        return "text"


@tool
def add_document_to_knowledge_base(
    file_path: str,
    chunk_size: Optional[int] = 1000,
    chunk_overlap: Optional[int] = 200,
    collection_name: Optional[str] = "knowledge_base",
    metadata: Optional[str] = None
) -> str:
    """
    æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆåŒ…å«åŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–å’Œå­˜å‚¨ï¼‰

    Args:
        file_path: æ–‡æ¡£è·¯å¾„ï¼ˆæ”¯æŒ .md, .docx æ ¼å¼ï¼‰
        chunk_size: æ–‡æœ¬å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼Œé»˜è®¤ 1000ï¼‰
        chunk_overlap: å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼Œé»˜è®¤ 200ï¼‰
        collection_name: å‘é‡é›†åˆåç§°ï¼ˆé»˜è®¤ knowledge_baseï¼‰
        metadata: æ–‡æ¡£å…ƒæ•°æ®ï¼ˆJSON å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            ä¾‹å¦‚: '{"category": "å»ºè´¦è§„åˆ™", "version": "1.0"}'

    Returns:
        å¤„ç†ç»“æœæ‘˜è¦ï¼ˆåŒ…æ‹¬æ–‡ä»¶ä¿¡æ¯ã€åˆ†å‰²ç»“æœã€å­˜å‚¨çŠ¶æ€ï¼‰

    Raises:
        ValueError: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å¤„ç†å¤±è´¥
    """
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(file_path):
        raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    # è·å–æ–‡ä»¶ä¿¡æ¯
    file_info = get_document_info(file_path)

    # åŠ è½½æ–‡æ¡£å†…å®¹
    try:
        content = load_document(file_path)
    except Exception as e:
        raise ValueError(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {str(e)}")

    # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åˆ†å‰²
    file_type = __get_file_type(file_path)

    try:
        if file_type == "markdown":
            # Markdown ä½¿ç”¨æ ‡é¢˜ç»“æ„åˆ†å‰²
            split_result = split_text_by_markdown_structure(content)
        else:
            # å…¶ä»–ä½¿ç”¨é€’å½’åˆ†å‰²
            split_result = split_text_recursive(
                content,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )

        # è§£æåˆ†å‰²ç»“æœä¸­çš„æ–‡æ¡£å—
        lines = split_result.split('\n')
        chunks = []
        current_chunk = ""
        for line in lines:
            if line.startswith('---'):
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = ""
            else:
                current_chunk += line + "\n"

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # åˆ›å»º Document å¯¹è±¡
        documents = []
        meta_data = json.loads(metadata) if metadata else {}
        base_metadata = {
            "source": os.path.basename(file_path),
            "file_path": file_path,
            "file_type": file_type,
        }
        base_metadata.update(meta_data)

        for i, chunk_text in enumerate(chunks):
            # è¿‡æ»¤æ‰éå†…å®¹è¡Œ
            if chunk_text.startswith(('---', 'ğŸ“', 'ğŸ“Š', '=')):
                continue

            doc = Document(
                page_content=chunk_text,
                metadata={
                    **base_metadata,
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                }
            )
            documents.append(doc)

        if not documents:
            raise ValueError(f"æ–‡æ¡£åˆ†å‰²åæ²¡æœ‰æœ‰æ•ˆçš„å†…å®¹å—: {file_path}")

        # å‘é‡åŒ–å¹¶å­˜å‚¨
        try:
            embeddings = get_embeddings()
            vector_store = get_vector_store(
                collection_name=collection_name,
                embeddings=embeddings
            )

            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            ids = vector_store.add_documents(documents)

        except Exception as e:
            raise RuntimeError(f"å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

        # è¿”å›ç»“æœ
        result = f"âœ… æ–‡æ¡£å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“\n\n"
        result += file_info + "\n"
        result += f"åˆ†å‰²å—æ•°: {len(documents)}\n"
        result += f"å‘é‡é›†åˆ: {collection_name}\n"
        result += f"æ–‡æ¡£ IDs: {ids[:10]}...\n"
        result += f"æ–‡æ¡£ ID æ€»æ•°: {len(ids)}\n"

        return result

    except Exception as e:
        raise RuntimeError(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")


@tool
def delete_documents_from_knowledge_base(
    source: Optional[str] = None,
    metadata_filter: Optional[str] = None,
    collection_name: Optional[str] = "knowledge_base"
) -> str:
    """
    ä»çŸ¥è¯†åº“åˆ é™¤æ–‡æ¡£

    Args:
        source: æºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        metadata_filter: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼ˆJSON å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            ä¾‹å¦‚: '{"category": "å»ºè´¦è§„åˆ™"}'
        collection_name: å‘é‡é›†åˆåç§°

    Returns:
        åˆ é™¤ç»“æœ

    Raises:
        ValueError: å¦‚æœå‚æ•°æ— æ•ˆ
    """
    try:
        vector_store = get_vector_store(collection_name=collection_name)

        # æ„å»ºåˆ é™¤æ¡ä»¶
        filters = {}

        if source:
            filters["source"] = source

        if metadata_filter:
            filter_data = json.loads(metadata_filter)
            filters.update(filter_data)

        if not filters:
            raise ValueError(
                "å¿…é¡»æä¾›è‡³å°‘ä¸€ä¸ªåˆ é™¤æ¡ä»¶ï¼ˆsource æˆ– metadata_filterï¼‰"
            )

        # æ‰§è¡Œåˆ é™¤
        # æ³¨æ„ï¼šPGVector çš„åˆ é™¤æ–¹æ³•å¯èƒ½éœ€è¦è°ƒæ•´
        # è¿™é‡Œä½¿ç”¨ delete æ–¹æ³•
        delete_count = vector_store.delete(where=filters)

        result = f"ğŸ—‘ï¸ æ–‡æ¡£åˆ é™¤ç»“æœ\n"
        result += f"åˆ é™¤æ¡ä»¶: {filters}\n"
        result += f"åˆ é™¤æ–‡æ¡£æ•°: {delete_count}\n"

        return result

    except Exception as e:
        raise RuntimeError(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")


@tool
def search_knowledge_base(
    query: str,
    k: Optional[int] = 5,
    collection_name: Optional[str] = "knowledge_base",
    score_threshold: Optional[float] = 0.7
) -> str:
    """
    ä»çŸ¥è¯†åº“æœç´¢ç›¸å…³æ–‡æ¡£

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        k: è¿”å›çš„æ–‡æ¡£æ•°ï¼ˆé»˜è®¤ 5ï¼‰
        collection_name: å‘é‡é›†åˆåç§°
        score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼Œé»˜è®¤ 0.7ï¼‰

    Returns:
        æœç´¢ç»“æœï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°å’Œå…ƒæ•°æ®ï¼‰

    Raises:
        ValueError: å¦‚æœæŸ¥è¯¢ä¸ºç©º
    """
    if not query or not query.strip():
        raise ValueError("æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")

    try:
        vector_store = get_vector_store(collection_name=collection_name)

        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        results = vector_store.similarity_search_with_score(
            query=query,
            k=k
        )

        # è¿‡æ»¤ä½åˆ†æ•°ç»“æœ
        filtered_results = [
            (doc, score) for doc, score in results
            if score >= score_threshold
        ]

        # æ ¼å¼åŒ–è¾“å‡º
        result = f"ğŸ” çŸ¥è¯†åº“æœç´¢ç»“æœ\n"
        result += f"æŸ¥è¯¢: {query}\n"
        result += f"è¿”å›ç»“æœæ•°: {len(filtered_results)}/{len(results)}\n"
        result += f"ç›¸ä¼¼åº¦é˜ˆå€¼: {score_threshold}\n"
        result += "=" * 50 + "\n\n"

        for i, (doc, score) in enumerate(filtered_results, 1):
            result += f"ã€ç»“æœ {i}ã€‘ç›¸ä¼¼åº¦: {score:.4f}\n"
            result += f"å†…å®¹: {doc.page_content[:300]}...\n"
            if doc.metadata:
                result += f"å…ƒæ•°æ®: {json.dumps(doc.metadata, ensure_ascii=False)}\n"
            result += "\n"

        if not filtered_results:
            result += "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆå¯èƒ½éœ€è¦é™ä½ score_thresholdï¼‰\n"

        return result

    except Exception as e:
        raise RuntimeError(f"æœç´¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}")


@tool
def get_knowledge_base_stats(
    collection_name: Optional[str] = "knowledge_base"
) -> str:
    """
    è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯

    Args:
        collection_name: å‘é‡é›†åˆåç§°

    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        # æ³¨æ„ï¼šPGVector å¯èƒ½æ²¡æœ‰ç›´æ¥çš„ç»Ÿè®¡æ–¹æ³•
        # è¿™é‡Œæˆ‘ä»¬è¿”å›åŸºç¡€ä¿¡æ¯
        result = f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡\n"
        result += f"é›†åˆåç§°: {collection_name}\n"
        result += f"çŠ¶æ€: å·²è¿æ¥\n"
        result += f"\næ³¨æ„: PGVector ä¸æä¾›ç›´æ¥çš„æ–‡æ¡£è®¡æ•°æ–¹æ³•ï¼Œ\n"
        result += f"å¯ä»¥é€šè¿‡æœç´¢æŸ¥è¯¢æ¥è·å–æ–‡æ¡£åˆ—è¡¨\n"

        return result

    except Exception as e:
        raise RuntimeError(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
