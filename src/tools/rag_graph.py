"""
LangGraph RAG èŠ‚ç‚¹
å®ç°å®Œæ•´çš„ RAG å·¥ä½œæµ
"""
from typing import TypedDict, Annotated, List, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
import os


# å®šä¹‰ RAG çŠ¶æ€
class RAGState(TypedDict):
    messages: Annotated[List, "æ¶ˆæ¯å†å²"]
    query: str  # ç”¨æˆ·æŸ¥è¯¢
    retrieved_docs: List[str]  # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    relevant_docs: List[str]  # ç›¸å…³æ–‡æ¡£ï¼ˆè¿‡æ»¤åï¼‰
    rewrite_query: str  # é‡å†™åçš„æŸ¥è¯¢
    answer: str  # ç”Ÿæˆçš„ç­”æ¡ˆ
    citations: List[str]  # å¼•ç”¨æ¥æº


def _get_llm():
    """è·å– LLM å®ä¾‹"""
    api_key = os.getenv("COZE_WORKLOAD_IDENTITY_API_KEY")
    base_url = os.getenv("COZE_INTEGRATION_MODEL_BASE_URL")

    llm = ChatOpenAI(
        model="doubao-seed-1-6-251015",  # ä½¿ç”¨é›†æˆæ¨¡å‹
        api_key=api_key,
        base_url=base_url,
        temperature=0.7,
        streaming=True,
        timeout=600
    )
    return llm


# èŠ‚ç‚¹ 1: æ£€ç´¢å†³ç­–èŠ‚ç‚¹
def retrieve_decision(state: RAGState) -> Literal["retrieve_docs", "direct_answer"]:
    """
    å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢æ–‡æ¡£

    åˆ¤æ–­é€»è¾‘ï¼š
    - å¦‚æœæŸ¥è¯¢åŒ…å«ç‰¹å®šå…³é”®è¯æˆ–éœ€è¦å¤–éƒ¨çŸ¥è¯†ï¼Œæ‰§è¡Œæ£€ç´¢
    - å¦åˆ™ç›´æ¥å›ç­”
    """
    query = state["query"]
    llm = _get_llm()

    # å†³ç­–æç¤ºè¯
    decision_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢å†³ç­–åŠ©æ‰‹ã€‚åˆ¤æ–­ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦éœ€è¦ä»çŸ¥è¯†åº“æ£€ç´¢æ–‡æ¡£ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
1. å¦‚æœæŸ¥è¯¢æ¶‰åŠå…·ä½“çš„ä¸šåŠ¡è§„åˆ™ã€æŠ€æœ¯ç»†èŠ‚ã€äº§å“ä¿¡æ¯ç­‰ï¼Œéœ€è¦æ£€ç´¢
2. å¦‚æœæ˜¯ç®€å•é—®å€™ã€é—²èŠã€å¸¸è¯†æ€§é—®é¢˜ï¼Œä¸éœ€è¦æ£€ç´¢

è¯·åªè¿”å› "retrieve" æˆ– "direct_answer"ã€‚
    """)

    user_query = HumanMessage(content=f"ç”¨æˆ·æŸ¥è¯¢: {query}")

    response = llm.invoke([decision_prompt, user_query])

    # è§£æå†³ç­–
    content = response.content
    decision = str(content).strip().lower() if isinstance(content, str) else ""

    if "retrieve" in decision:
        return "retrieve_docs"
    else:
        return "direct_answer"


# èŠ‚ç‚¹ 2: æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹
def retrieve_docs(state: RAGState) -> RAGState:
    """
    ä»çŸ¥è¯†åº“æ£€ç´¢æ–‡æ¡£
    """
    query = state["query"]
    # ç¡®ä¿ query æ˜¯å­—ç¬¦ä¸²
    if not isinstance(query, str):
        query = str(query)

    # å¯¼å…¥ RAG æ£€ç´¢å·¥å…·
    from tools.rag_retriever import rag_retrieve_with_rerank

    # æ‰§è¡Œæ£€ç´¢
    retrieval_result = rag_retrieve_with_rerank(
        query=query,
        initial_k=20,
        top_n=5,
        use_rerank=True
    )

    # è§£ææ£€ç´¢ç»“æœï¼ˆç®€åŒ–å¤„ç†ï¼‰
    # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥è§£æè¿”å›çš„å­—ç¬¦ä¸²æå–æ–‡æ¡£
    retrieved_docs = [retrieval_result]

    return {"retrieved_docs": retrieved_docs}


# èŠ‚ç‚¹ 3: æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°èŠ‚ç‚¹
def grade_documents(state: RAGState) -> Literal["generate_answer", "rewrite_query"]:
    """
    è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³

    ç›¸å…³æ€§æ ‡å‡†ï¼š
    - æ–‡æ¡£æ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
    - æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯
    """
    query = state["query"]
    retrieved_docs = state["retrieved_docs"]
    llm = _get_llm()

    # è¯„ä¼°æç¤ºè¯
    grade_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°åŠ©æ‰‹ã€‚è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ç›¸å…³ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. æ–‡æ¡£æ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
2. æ–‡æ¡£å†…å®¹æ˜¯å¦å‡†ç¡®ã€å®Œæ•´
3. æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯

è¯·è¿”å›ï¼š
- å¦‚æœæ–‡æ¡£ç›¸å…³ï¼šè¿”å› "relevant"
- å¦‚æœæ–‡æ¡£ä¸ç›¸å…³ï¼šè¿”å› "not_relevant"
    """)

    # æ„å»ºè¯„ä¼°è¾“å…¥
    docs_text = "\n\n".join(retrieved_docs)
    grade_input = HumanMessage(content=f"""
ç”¨æˆ·æŸ¥è¯¢: {query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{docs_text}

è¿™äº›æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼Ÿè¯·è¿”å› "relevant" æˆ– "not_relevant"ã€‚
    """)

    response = llm.invoke([grade_prompt, grade_input])
    content = response.content
    evaluation = str(content).strip().lower() if isinstance(content, str) else ""

    if "relevant" in evaluation:
        # ç®€åŒ–ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä½œä¸ºç›¸å…³æ–‡æ¡£
        return {"relevant_docs": retrieved_docs}
    else:
        return "rewrite_query"


# èŠ‚ç‚¹ 4: é—®é¢˜é‡å†™èŠ‚ç‚¹
def rewrite_query(state: RAGState) -> RAGState:
    """
    é‡å†™ç”¨æˆ·æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢æ•ˆæœ
    """
    query = state["query"]
    llm = _get_llm()

    # é‡å†™æç¤ºè¯
    rewrite_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢é‡å†™åŠ©æ‰‹ã€‚é‡å†™ç”¨æˆ·æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢æ•ˆæœã€‚

é‡å†™åŸåˆ™ï¼š
1. ä¿æŒæŸ¥è¯¢çš„åŸå§‹æ„å›¾
2. ä½¿æŸ¥è¯¢æ›´æ¸…æ™°ã€æ›´å…·ä½“
3. æ·»åŠ ç›¸å…³çš„å…³é”®è¯
4. ä¸è¦æ”¹å˜ç”¨æˆ·çš„é—®é¢˜

è¯·ç›´æ¥è¿”å›é‡å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚
    """)

    user_query = HumanMessage(content=f"åŸå§‹æŸ¥è¯¢: {query}")
    response = llm.invoke([rewrite_prompt, user_query])

    content = response.content
    rewrite_query = str(content).strip() if isinstance(content, str) else ""

    return {"rewrite_query": rewrite_query}


# èŠ‚ç‚¹ 5: ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹ï¼ˆå¸¦å¼•ç”¨ï¼‰
def generate_answer(state: RAGState) -> RAGState:
    """
    åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶æä¾›å¼•ç”¨
    """
    query = state["query"]
    relevant_docs = state.get("relevant_docs", state.get("retrieved_docs", []))
    llm = _get_llm()

    # ç”Ÿæˆæç¤ºè¯
    answer_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹
2. ä¸è¦ç¼–é€ ä¿¡æ¯
3. ç­”æ¡ˆè¦å‡†ç¡®ã€æ¸…æ™°ã€ç»“æ„åŒ–
4. å¿…é¡»åœ¨ç­”æ¡ˆæœ«å°¾æä¾›å¼•ç”¨æ¥æº
5. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜

å¼•ç”¨æ ¼å¼ï¼š
---
å¼•ç”¨æ¥æº:
[æ¥æº1]
[æ¥æº2]
---
    """)

    # æ„å»ºä¸Šä¸‹æ–‡
    docs_text = "\n\n".join(relevant_docs)

    answer_input = HumanMessage(content=f"""
ç”¨æˆ·é—®é¢˜: {query}

çŸ¥è¯†åº“å†…å®¹:
{docs_text}

è¯·åŸºäºä»¥ä¸Šå†…å®¹å›ç­”é—®é¢˜ï¼Œå¹¶æä¾›å¼•ç”¨ã€‚
    """)

    response = llm.invoke([answer_prompt, answer_input])
    content = response.content
    answer = str(content).strip() if isinstance(content, str) else ""

    # æå–å¼•ç”¨ï¼ˆç®€åŒ–å¤„ç†ï¼‰
    # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥è§£æç­”æ¡ˆä¸­çš„å¼•ç”¨éƒ¨åˆ†
    citations = ["[çŸ¥è¯†åº“]"]  # ç®€åŒ–å¤„ç†

    return {
        "answer": answer,
        "citations": citations
    }


# èŠ‚ç‚¹ 6: åç»­é—®é¢˜å»ºè®®èŠ‚ç‚¹
def suggest_questions(state: RAGState) -> RAGState:
    """
    åŸºäºå½“å‰é—®é¢˜å’Œç­”æ¡ˆï¼Œç”Ÿæˆåç»­é—®é¢˜å»ºè®®
    """
    query = state["query"]
    answer = state.get("answer", "")
    llm = _get_llm()

    # ç”Ÿæˆæç¤ºè¯
    suggest_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªé—®é¢˜å»ºè®®åŠ©æ‰‹ã€‚åŸºäºç”¨æˆ·çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œç”Ÿæˆ3ä¸ªç›¸å…³çš„åç»­é—®é¢˜ã€‚

åç»­é—®é¢˜è¦æ±‚ï¼š
1. ä¸å½“å‰é—®é¢˜å’Œç­”æ¡ˆç›¸å…³
2. æœ‰åŠ©äºç”¨æˆ·æ·±å…¥äº†è§£ç›¸å…³è¯é¢˜
3. å…·ä½“ä¸”æœ‰æ¢ç´¢ä»·å€¼
4. ä¸è¦é‡å¤å½“å‰é—®é¢˜

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š
{
  "suggested_questions": [
    "é—®é¢˜1",
    "é—®é¢˜2",
    "é—®é¢˜3"
  ]
}
    """)

    suggest_input = HumanMessage(content=f"""
ç”¨æˆ·é—®é¢˜: {query}

AI ç­”æ¡ˆ: {answer[:500]}...

è¯·ç”Ÿæˆ3ä¸ªåç»­é—®é¢˜å»ºè®®ã€‚
    """)

    response = llm.invoke([suggest_prompt, suggest_input])

    # è§£æåç»­é—®é¢˜ï¼ˆç®€åŒ–å¤„ç†ï¼‰
    # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥è§£æ JSON
    content = response.content
    suggested_questions = str(content).strip() if isinstance(content, str) else ""

    # æ·»åŠ åˆ°ç­”æ¡ˆä¸­
    final_answer = f"{state.get('answer', '')}\n\n---\nğŸ’¡ åç»­é—®é¢˜å»ºè®®:\n{suggested_questions}"

    return {"answer": final_answer}


def build_rag_graph():
    """
    æ„å»º RAG LangGraph

    Returns:
        LangGraph å®ä¾‹
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("retrieve_decision", retrieve_decision)
    workflow.add_node("retrieve_docs", retrieve_docs)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("rewrite_query", rewrite_query)
    workflow.add_node("generate_answer", generate_answer)
    workflow.add_node("suggest_questions", suggest_questions)

    # æ·»åŠ è¾¹
    workflow.add_edge(START, "retrieve_decision")
    workflow.add_conditional_edges(
        "retrieve_decision",
        {
            "retrieve_docs": "retrieve_docs",
            "direct_answer": "generate_answer"
        }
    )
    workflow.add_edge("retrieve_docs", "grade_documents")
    workflow.add_conditional_edges(
        "grade_documents",
        {
            "generate_answer": "generate_answer",
            "rewrite_query": "rewrite_query"
        }
    )
    workflow.add_edge("rewrite_query", "retrieve_docs")  # é‡æ–°æ£€ç´¢
    workflow.add_edge("generate_answer", "suggest_questions")
    workflow.add_edge("suggest_questions", END)

    # ç¼–è¯‘å›¾
    app = workflow.compile()

    return app


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ„å»ºå›¾
    rag_app = build_rag_graph()

    # æµ‹è¯•è¿è¡Œ
    test_state = {
        "query": "ä»€ä¹ˆæ˜¯å»ºè´¦è§„åˆ™ï¼Ÿ",
        "messages": []
    }

    # æ‰§è¡Œå›¾
    result = rag_app.invoke(test_state)

    print("=== RAG æµç¨‹ç»“æœ ===")
    print(result)
